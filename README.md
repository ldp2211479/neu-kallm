## Environment

å…‹éš†ä»“åº“ï¼š

```
git clone 
cd 
```

åˆ›å»ºPythonç¯å¢ƒï¼š

```
conda env create -f requirements.yaml
```

æ¿€æ´»ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–åŒ…

```
conda activate kg
python -m spacy download en_core_web_sm
```

æ— è®ºä½•æ—¶è¿è¡Œä»¥ä¸‹ä»»ä½•å‘½ä»¤ï¼Œè¯·ç¡®ä¿æ­¤ç¯å¢ƒéƒ½å·²æ¿€æ´»ã€‚

## Model and Datasets Download



## Directory Structure

ä»¥ä¸‹ä¸ºå®é™…è¿è¡Œçš„ç›®å½•ç»“æ„

- model
	- 
- datasets
	- tiq
	- timequestions
- colbert
	- model
		- wikipedia_11_06_2023
		- colbert_checkpoint
- outputs
- utils





## ğŸ˜‹How to run

#### 1. Running a retrieval service

è¿›å…¥ç¯å¢ƒ

```bash
conda activate ldy_kg_wikichat
```

**Wikipedia Text and Table**

é¦–å…ˆï¼Œä¸ºäº†ä»ç»´åŸºç™¾ç§‘æ£€ç´¢æ–‡æœ¬ä¸è¡¨æ ¼ï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹æŒ‡ä»¤æ¥å¯åŠ¨ä¸€ä¸ªColbertæœåŠ¡å™¨ï¼Œä»¥å¯¹è¯æ®è¿›è¡Œæ£€ç´¢ã€‚å»ºè®®æ‚¨é€šè¿‡ **tmux** **/ screen** ç­‰åå°ç¨‹åºå¯åŠ¨ä¸€ä¸ªæœåŠ¡å™¨åå°ï¼Œæ¥æŒç»­ç›‘å¬ä¿¡å·ã€‚

```
cd colbert
bash colbert_app.sh
```

ä½¿ç”¨ ColBERT ä¸éœ€è¦ GPUï¼Œå› ä¸ºå®ƒè®¾ç½®ä¸ºä½¿ç”¨ CPUã€‚æ•´ä¸ªç´¢å¼•å°†åŠ è½½åˆ° RAMï¼Œè¿™éœ€è¦å¤§çº¦ 100GB çš„ RAMã€‚å¦‚æœæ‚¨æ²¡æœ‰é‚£ä¹ˆå¤š RAMï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ `colbert_memory_map=true`åˆ°æ­¤å‘½ä»¤æ¥å¯ç”¨å†…å­˜æ˜ å°„ã€‚è¿™ä¼šå°† RAM ä½¿ç”¨é‡å‡å°‘åˆ°å¤§çº¦ 35GBï¼Œä½†ä¼šä½¿æ£€ç´¢é€Ÿåº¦å˜æ…¢ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨ç›‘å¬ç«¯å£ 5000ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œå¦‚ä¸‹ curl å‘½ä»¤æ¥æµ‹è¯•æ­¤æœåŠ¡å™¨ï¼š

```bash
curl http://127.0.0.1:5000/search -d '{"query": "who is the current monarch of the united kingdom?", "evi_num": 1, "source": "wikipedia"}' -X GET -H 'Content-Type: application/json'
```

**Wikidata**



é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨ç›‘å¬ç«¯å£ 5000ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œå¦‚ä¸‹ curl å‘½ä»¤æ¥æµ‹è¯•æ­¤æœåŠ¡å™¨ï¼š

```bash
curl http://127.0.0.1:5000/search -d '{"query": "what is the political system in argentina?", "source":"wikidata"}' -X GET -H 'Content-Type: application/json'
```



#### 2. Inference

è¿›å…¥ç¯å¢ƒ

```
conda activate ldy_kg_cok
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨ Ollama æ¡†æ¶æ¥è¿›è¡Œæµ‹è¯•ã€‚åœ¨ linux ç³»ç»Ÿä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥ä¸‹è½½ Ollama æ¡†æ¶ï¼Œå¹¶è¿è¡Œä¸€ä¸ªæœ¬åœ°çš„å¤§æ¨¡å‹ï¼Œè¿™éå¸¸æ–¹ä¾¿å¿«æ·ã€‚ 

```bash
curl -fsSL https://ollama.com/install.sh | sh
Ollama run llama3:70b
```

åœ¨ç¡®ä¿æ‚¨å·²ç»å¼€å¯äº†ä¸Šè¿°æ£€ç´¢æœåŠ¡å™¨çš„å‰æä¸‹ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹ sh è„šæœ¬æ¥è¿›è¡Œæ¨ç†ï¼š

```
bash run.sh
```

é™¤æ­¤ä¹‹å¤–ï¼Œæ‚¨å¯ä»¥é€‰æ‹©é‡‡ç”¨ gpt-3.5-turbo-0613 ã€ GPT4 ç­‰æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚åˆ›å»ºè´¦æˆ·å¹¶è·å– OpenAI çš„ API å¯†é’¥ (https://openai.com)ï¼Œå¹¶å¯¹è„šæœ¬è¿›è¡Œä»¥ä¸‹æ›´æ”¹ï¼š

```bash
export TRANSFORMERS_VERBOSITY=error
export OPENAI_API_KEY=<YOUR_KEY>
DATASET="timequestions"
TYPE="train"
MODEL="gpt-3.5-turbo-0613"

python run.py \
    --model ${MODEL} \
    --dataset ${DATASET} \
    --input datasets/${DATASET}/${DATASET}_${TYPE}.json \
    --output outputs/${DATASET}/${DATASET}_${TYPE}_out.json
```



#### **eval**
