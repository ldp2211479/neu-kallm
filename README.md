

## Environment

Clone:

```
git clone 
cd 
```

#### Create environment for inference

Create a Python environment:

```
conda env create -f environments.yml
```

Activate the environment and install dependency packagesï¼š

```
conda activate kallm
python -m spacy download en_core_web_sm
```

#### Create environment for Retrieval

Create a Python environment:

```
cd server
conda env create -f server_env.yml
```

Activate the environment and install dependency packagesï¼š

```
conda activate kg_server
```



## Model and Datasets Download

#### Running ReFiNED model for Wikidata retrieval

Download the fine-tuned ReFiNED model by:

```
pip install https://github.com/amazon-science/ReFinED/archive/refs/tags/V1.zip 
mkdir -p <your_directory>
curl https://almond-static.stanford.edu/research/qald/refined-finetune/config.json -o <your_directory>/config.json
curl https://almond-static.stanford.edu/research/qald/refined-finetune/model.pt -o <your_directory>/model.pt
curl https://almond-static.stanford.edu/research/qald/refined-finetune/precomputed_entity_descriptions_emb_wikidata_33831487-300.np -o <your_directory>/precomputed_entity_descriptions_emb_wikidata_33831487-300.np
```

By default, I recommend you download it to `~/server/model/wikidata/refined`.



## Directory Structure

ä»¥ä¸‹ä¸ºå®é™…è¿è¡Œçš„ç›®å½•ç»“æ„ï¼š

```
-- datasets
-- outputs
-- server
    -- ColBERT
    -- dataset ï¼ˆåˆ›å»ºWikipediaå’ŒWikitableæ£€ç´¢çš„ä¸­é—´æ–‡ä»¶ï¼‰
        -- wikipedia
        -- wikitable
    -- model
        -- wikidata
            -- llama-7b-wikiwebquestions-qald7
            -- refined
        -- wikipedia
            -- indexes
            -- colbert_checkpoint
            collection.tsv
        -- wikitable
            -- indexes
            -- colbert_checkpoint
            collection.tsv
    -- prompts ï¼ˆWikidataæ£€ç´¢ä¸‹LLAMAæ¨¡å‹çš„æç¤ºæ¨¡æ¿ï¼‰
    -- script ï¼ˆåˆ›å»ºWikipediaå’ŒWikitableæ£€ç´¢çš„è„šæœ¬æ–‡ä»¶ï¼‰
    -- utils
        -- wikitable
        -- wikipedia
        -- wikidata
        globalvar.py
    server_app.py
-- utils
    -- parser ï¼ˆæ•°æ®è§£æå™¨ï¼‰
    -- retrieval ï¼ˆæ¨ç†å™¨ä¸æ£€ç´¢æœåŠ¡å™¨çš„äº¤äº’å‡½æ•°ï¼‰
    eval_utils.py
    globalvar.py
    knowl_query.py
    openai_utils.py
    other_prompts.py
eval.py
run.py
```



## ğŸ˜‹How to run

#### 1. Running a retrieval service

è¿›å…¥ç¯å¢ƒ

```bash
conda activate ldy_kg_wikichat
```

é¦–å…ˆï¼Œä¸ºäº†ä»ç»´åŸºç™¾ç§‘æ£€ç´¢æ–‡æœ¬ä¸è¡¨æ ¼ï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹æŒ‡ä»¤æ¥å¯åŠ¨ä¸€ä¸ªæœåŠ¡å™¨ï¼Œä»¥å¯¹è¯æ®è¿›è¡Œæ£€ç´¢ã€‚å»ºè®®æ‚¨é€šè¿‡ **tmux** **/ screen** ç­‰åå°ç¨‹åºå¯åŠ¨ä¸€ä¸ªæœåŠ¡å™¨åå°ï¼Œæ¥æŒç»­ç›‘å¬ä¿¡å·ã€‚

```
cd colbert
bash colbert_app.sh
```

è¯¥æœåŠ¡å™¨ä¸­å­˜åœ¨ä¸‰ä¸ªå‚æ•°ï¼Œé€šè¿‡`source`å‚æ•°æ¥è¿›è¡Œæ£€ç´¢æºçš„é€‰æ‹©ã€‚

**Wikipedia Text and Table**

ä½¿ç”¨ ColBERT ä¸éœ€è¦ GPUï¼Œå› ä¸ºå®ƒè®¾ç½®ä¸ºä½¿ç”¨ CPUã€‚æ•´ä¸ªç´¢å¼•å°†åŠ è½½åˆ° RAMï¼Œè¿™éœ€è¦å¤§çº¦ 100GB çš„ RAMã€‚å¦‚æœæ‚¨æ²¡æœ‰é‚£ä¹ˆå¤š RAMï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ `colbert_memory_map=true`åˆ°æ­¤å‘½ä»¤æ¥å¯ç”¨å†…å­˜æ˜ å°„ã€‚è¿™ä¼šå°† RAM ä½¿ç”¨é‡å‡å°‘åˆ°å¤§çº¦ 35GBï¼Œä½†ä¼šä½¿æ£€ç´¢é€Ÿåº¦å˜æ…¢ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨ç›‘å¬ç«¯å£ 5000ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œå¦‚ä¸‹ curl å‘½ä»¤æ¥æµ‹è¯•æ­¤æœåŠ¡å™¨ï¼š

```bash
curl http://127.0.0.1:5000/search -d '{"query": "who is the current monarch of the united kingdom?", "evi_num": 1, "source": "wikipedia"}' -X GET -H 'Content-Type: application/json'
```

**Wikitable and Infobox**

é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨ç›‘å¬ç«¯å£ 5000ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œå¦‚ä¸‹ curl å‘½ä»¤æ¥æµ‹è¯•æ­¤æœåŠ¡å™¨ï¼š

```bash
curl http://127.0.0.1:5000/search -d '{"query": "who is the current monarch of the united kingdom?", "evi_num": 1, "source": "wikitable"}' -X GET -H 'Content-Type: application/json'
```

**Wikidata**

é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨ç›‘å¬ç«¯å£ 5000ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œå¦‚ä¸‹ curl å‘½ä»¤æ¥æµ‹è¯•æ­¤æœåŠ¡å™¨ï¼š

```bash
curl http://127.0.0.1:5000/search -d '{"query": "what is the political system in argentina?", "source":"wikidata"}' -X GET -H 'Content-Type: application/json'
```

#### 2. Inference

è¿›å…¥ç¯å¢ƒ

```
conda activate ldy_kg_cok
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨ Ollama æ¡†æ¶æ¥è¿›è¡Œæµ‹è¯•ã€‚åœ¨ Linux ç³»ç»Ÿä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥ä¸‹è½½ Ollama æ¡†æ¶ï¼Œå¹¶è¿è¡Œä¸€ä¸ªæœ¬åœ°çš„å¤§æ¨¡å‹ï¼Œè¿™éå¸¸æ–¹ä¾¿å¿«æ·ã€‚ 

```
curl -fsSL https://ollama.com/install.sh | sh
Ollama run llama3:70b
```

åœ¨ç¡®ä¿æ‚¨å·²ç»å¼€å¯äº†ä¸Šè¿°æ£€ç´¢æœåŠ¡å™¨çš„å‰æä¸‹ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹ sh è„šæœ¬æ¥è¿›è¡Œæ¨ç†ï¼š

```
bash run.sh
```

é™¤æ­¤ä¹‹å¤–ï¼Œæ‚¨å¯ä»¥é€‰æ‹©é‡‡ç”¨ gpt-3.5-turbo-0613 ã€ GPT4 ç­‰æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚åˆ›å»ºè´¦æˆ·å¹¶è·å– OpenAI çš„ API å¯†é’¥ (https://openai.com)ï¼Œå¹¶å¯¹è„šæœ¬è¿›è¡Œä»¥ä¸‹æ›´æ”¹ï¼š

```
export TRANSFORMERS_VERBOSITY=error
export OPENAI_API_KEY=<YOUR_KEY>
DATASET="timequestions"
TYPE="train"
MODEL="gpt-3.5-turbo-0613"

python run.py \
    --model ${MODEL} \
    --dataset ${DATASET} \
    --input datasets/${DATASET}/${DATASET}_${TYPE}.json \
    --output outputs/${DATASET}/${DATASET}_${TYPE}_out.json
```



## ğŸ¤¯Creating Colbert Indexes

#### 1. Download the Wikipedia dump

è¿›å…¥ `server` ä¸­ï¼Œè¿è¡Œä»¥ä¸‹è„šæœ¬æ–‡ä»¶ä»¥ä¸‹è½½æœ€æ–°çš„wikipedia dumpsã€‚

```
bash script/download_wikipedia_dumps.sh
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ–‡ä»¶`enwiki-latest-pages-articles.xml.bz2`ä¼šä¸‹è½½åœ¨`~/server/dataset`ä¸­ã€‚å¦‚æœæ‚¨éœ€è¦ï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹scriptæ–‡ä»¶å¤¹ä¸­çš„è„šæœ¬é…ç½®ã€‚

#### 2. Wikipedia

The following script defaults the wikipedia dump to the directory `~/server/dataset/enwiki-latest-pages-articles.xml.bz2`.

Run the Wikipedia parserï¼š

```
bash script/wikipedia_parser.sh
```

This will extract the pages into a set of sharded files, which will be located in the text/ directory. This step takes a few hours.

Run the splitterï¼š

```
bash script/wikipedia_split_passages.sh
```

This script will split the Wikipedia documents into blocks, with each block containing up to `max_block_words` words. It will write these blocks into `collection_all.tsv` which is then used for making the ColBERT index.

Run this command to start ColBERT indexing. This step should be run on GPUs:

```
bash script/wikipedia_index_split.sh
```

Optionally, you can merge all the smaller index files into a single file. This will enable you to use mempory mapping when loading the index, which will reduce the RAM usage at the cost of increasing the retrieval speed. This step requires enough RAM to load the entire index, about 100GB of RAM, but after that, the resulting index can be run on a machine with as little as 35GB of RAM.

```
bash script/wikipedia_coalesce_index.sh
```

#### 3. Wikitable and Infobox

The following script defaults the wikipedia dump to the directory `~/server/dataset/enwiki-latest-pages-articles.xml.bz2`.

Run the Wikitable and Infobox parserï¼š

```
bash script/wikitable_parser.sh
```

By running [wikitextparser](https://github.com/5j9/wikitextparser) and regular expression, this will parse the Wikipedia page into a textualized table.

Run this command to generate ColBERT collection and start ColBERT indexing. This step should be run on GPUs:

```
bash script/wikitable_index_split.sh
```

Optionally, you can merge all the smaller index files into a single file. This will enable you to use mempory mapping when loading the index, which will reduce the RAM usage at the cost of increasing the retrieval speed. 

```
bash script/wikitable_coalesce_index.sh
```



## Evaluate

